Various ways of discovering hidden or private content on a webserver that could lead to new vulnerabilities.

---

Firstly, we should ask, in the context of web application security, what is content? Content can be many things, a file, video, picture, backup, a website feature. When we talk about content discovery, we're not talking about the obvious things we can see on a website; it's the things that aren't immediately presented to us and that weren't always intended for public access.  
  
This content could be, for example, pages or portals intended for staff usage, older versions of the website, backup files, configuration files, administration panels, etc.  
  
There are three main ways of discovering content on a website which we'll cover. Manually, Automated andÂ OSINTÂ (Open-Source Intelligence).

---

## Manual Discovery

There are multiple places we can manually check on a website to start discovering more content.Â 

### Robots.txt

==**`robots.txt`** is a simple text file that websites use to give instructions to web crawlers (like Googlebot, Bingbot, or other search engine bots) about which pages or files theyâ€™re allowed or not allowed to crawl and index.==

Itâ€™s part of the **Robots Exclusion Protocol (REP)** â€” a standard for managing crawler access to your site.

**Key points about `robots.txt`:**
- ğŸ“ **Location:** It must be placed in the root directory of your website. For example, `https://example.com/robots.txt`.
- âš™ï¸ **Syntax:** It uses simple rules like `User-agent` to specify which bot the rule applies to, and `Disallow` or `Allow` to tell bots what they can or canâ€™t crawl.
- ğŸš« **Purpose:** Common uses include blocking private areas (like `/admin/`), preventing indexing of duplicate content, or managing crawl budgets for large sites.
- â— **Limitations:** Itâ€™s not a security feature â€” it only _requests_ bots to stay away; it doesnâ€™t prevent them from accessing the content if they choose to ignore the rules.


**Example `robots.txt`:**

```txt
User-agent: *
Disallow: /private/
Allow: /public/
```

This means:
- `User-agent: *` â†’ applies to all bots.
- `Disallow: /private/` â†’ bots should not crawl anything under `/private/`.
- `Allow: /public/` â†’ bots can crawl `/public/`.


This file gives us a great list of locations on the website that the owners don't want us to discover as penetration testers.

Take a look at the robots.txt file on the Acme IT Support website to see if they have anything they don't want to list - To do this open Firefox on the AttackBox, and enter the url:Â [http://MACHINE_IP/robots.txt](http://machine_ip/robots.txt)[](https://lab_web_url.p.thmlabs.com/robots.txt)Â (_this URL will update 2 minutes from when you start the machine in task 1_)

---

---

---

## References

https://tryhackme.com/room/contentdiscovery